{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65e988f",
   "metadata": {},
   "source": [
    "When looking at the training loop, I noticed something odd.\n",
    "\n",
    "```\n",
    "y_pred = model_0(X_train)\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "How does the `loss.backward()` step allow the optimizer to update the weights/biases accordingly?\n",
    "There is seemingly no reference between loss and optimizer. The optimizer has the model since we initialized it with the model parameters earlier, but the loss is generated simply from y_pred and y_train, two tensors. So how does the loss get communicated to the model?\n",
    "\n",
    "The answer lies in PyTorch's autograd mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "30ecc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62bd0301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5bfc176250>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31bb693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
    "                                                dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                   requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
    "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7f4b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad_graph(tensor, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the computation graph of a tensor's gradient function.\n",
    "    \"\"\"\n",
    "    if hasattr(tensor, 'grad_fn') and tensor.grad_fn is not None:\n",
    "        print(\" \" * indent + f\"{tensor.grad_fn}\")\n",
    "        for f, _ in tensor.grad_fn.next_functions:\n",
    "            if f is not None:\n",
    "                print_grad_graph(f, indent + 4)\n",
    "    elif hasattr(tensor, 'next_functions'):\n",
    "        print(\" \" * indent + f\"{tensor}\")\n",
    "        for f, _ in tensor.next_functions:\n",
    "            if f is not None:\n",
    "                print_grad_graph(f, indent + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54025707",
   "metadata": {},
   "source": [
    "The grad_fn attribute of a tensor shows the last operation that created the tensor.\n",
    "\n",
    "Since our linear regression model is just y = mx+b, we should see an add operation here since adding bias is done last.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "02da8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.grad_fn: <AddBackward0 object at 0x7f5adac7ce20>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LinearRegressionModel()\n",
    "x = torch.randn(1,1, requires_grad=False)\n",
    "y_pred = model(x)\n",
    "\n",
    "print(f\"y_pred.grad_fn: {y_pred.grad_fn}\")  # Points to AddBackward0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221c979",
   "metadata": {},
   "source": [
    "If we want to see more the operations before the last one, we can use the .next_functions attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c7b4083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.grad_fn.next_functions: ((<MulBackward0 object at 0x7f5adac820e0>, 0), (<AccumulateGrad object at 0x7f5adac80fd0>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_pred.grad_fn.next_functions: {y_pred.grad_fn.next_functions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f5951",
   "metadata": {},
   "source": [
    "Here we see two items, the multiplication operation and an 'AccumulateGrad' operation.\n",
    "\n",
    "The 'AccumulateGrad' object is a place to store the gradient for the add operation, which is related to the bias.\n",
    "\n",
    "We can prove this by showing that the variable in the 'AccumulateGrad' object is the bias tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1ff98af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.grad_fn.next_functions[1][0].variable:\n",
      "Parameter containing:\n",
      "tensor([-0.2934], requires_grad=True)\n",
      "\n",
      "Bias tensor from model:\n",
      "Parameter containing:\n",
      "tensor([-0.2934], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_pred.grad_fn.next_functions[1][0].variable:\\n{y_pred.grad_fn.next_functions[1][0].variable}\\n\")  # Should show bias tensor\n",
    "print(f\"Bias tensor from model:\\n{model.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2337c67",
   "metadata": {},
   "source": [
    "Similarly to the bias, we can see the gradient for the weight as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2f567288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_fn: <MulBackward0 object at 0x7f5adac81270>\n",
      "((<AccumulateGrad object at 0x7f5adac822c0>, 0), (None, 0))\n",
      "\n",
      "weight_fn.next_functions[0][0].variable:\n",
      "Parameter containing:\n",
      "tensor([1.5410], requires_grad=True)\n",
      "\n",
      "Weights tensor from model:\n",
      "Parameter containing:\n",
      "tensor([1.5410], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "weight_fn = y_pred.grad_fn.next_functions[0][0]\n",
    "print(f\"weight_fn: {weight_fn}\")  # Should point to MulBackward0\n",
    "print(weight_fn.next_functions)\n",
    "print()\n",
    "\n",
    "print(f\"weight_fn.next_functions[0][0].variable:\\n{weight_fn.next_functions[0][0].variable}\\n\")  # Should show weights tensor\n",
    "print(f\"Weights tensor from model:\\n{model.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9516601b",
   "metadata": {},
   "source": [
    "Notice that there is another value in the next_functions tuple for the weight function that is None.\n",
    "\n",
    "This is actually representing the input data X. This value is None because the X tensor does not have autograd on.\n",
    "\n",
    "If we want to see a non-None value there, we can turn on requires_grad for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1f9e7e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplication op next_functions: ((<AccumulateGrad object at 0x7f5adabbbd30>, 0), (<AccumulateGrad object at 0x7f5adac82080>, 0))\n",
      "\n",
      "Input object from multiplication op:\n",
      "tensor([[0.5684]], requires_grad=True)\n",
      "\n",
      "Input tensor from model:\n",
      "tensor([[0.5684]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_with_grad = torch.randn(1,1, requires_grad=True)\n",
    "y_pred_with_grad = model(x_with_grad)\n",
    "\n",
    "print(f\"Multiplication op next_functions: {y_pred_with_grad.grad_fn.next_functions[0][0].next_functions}\")\n",
    "print(f\"\\nInput object from multiplication op:\\n{y_pred_with_grad.grad_fn.next_functions[0][0].next_functions[1][0].variable}\\n\")  # Should show x tensor\n",
    "print(f\"Input tensor from model:\\n{x_with_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5aca8",
   "metadata": {},
   "source": [
    "All of this is PyTorch's autograd mechanism, for more details see here: https://docs.pytorch.org/docs/stable/notes/autograd.html.\n",
    "\n",
    "This mechanism is how `loss.backwards()` can still allow the optimizer to update weights/biases, as the gradient gets stored in the parameters themselves.\n",
    "\n",
    "Here we can see the full tree of the loss. Reminder that L1 Loss used here is the mean of |y_pred-y_true|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "88f2f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward0 object at 0x7f5adabbb550>\n",
      "    <AbsBackward0 object at 0x7f5adac82050>\n",
      "        <SubBackward0 object at 0x7f5adac979a0>\n",
      "            <AddBackward0 object at 0x7f5adac97460>\n",
      "                <MulBackward0 object at 0x7f5adac974c0>\n",
      "                    <AccumulateGrad object at 0x7f5adac97a30>\n",
      "                <AccumulateGrad object at 0x7f5adac975b0>\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "x = torch.randn(1, 1)\n",
    "loss_fn = nn.L1Loss()\n",
    "y_true = torch.randn(1, 1)\n",
    "\n",
    "y_pred = model(x)\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "print_grad_graph(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lesson_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
